
# ğŸµ **Spotify ETL Pipeline using Apache Airflow**  

This project is an **ETL (Extract, Transform, Load) pipeline** for **Spotify playlist data**, built using **Apache Airflow**. It automates the process of:  

- **Extracting** playlist track data from **Spotify API**  
- **Transforming** the data into a structured format using **PySpark**  
- **Loading** the transformed data into **AWS S3** for further analysis  

---
![Spotify Etl Pipeline](https://github.com/shivatadha/Spotify-ETL-Pipeline-using-Apache-Airflow/blob/185bebdc5dfabe069a4d1e80217d9a3d08345b0a/Spotify%20etl%20pipeline.png)
---

## ğŸš€ **Features**  

âœ”ï¸ **Extract** â†’ Fetches playlist track data from **Spotify API**  
âœ”ï¸ **Transform** â†’ Processes and cleans data using **Apache Spark**  
âœ”ï¸ **Load** â†’ Stores transformed data in **AWS S3** as **Parquet**  
âœ”ï¸ **Cleanup** â†’ Deletes temporary files after processing  
âœ”ï¸ **Automation** â†’ Uses **Apache Airflow DAGs** for scheduling  

---

## ğŸ“‚ **Project Structure**  

```plaintext
spotify-etl-pipeline/
â”‚â”€â”€ dags/                         # Airflow DAGs and ETL scripts
â”‚   â”œâ”€â”€ fetch_spotify_data.py     # Extracts data from Spotify API
â”‚   â”œâ”€â”€ spotify_transformation.py # Transforms raw JSON using PySpark
â”‚   â”œâ”€â”€ parquet_writer.py         # Saves transformed data as Parquet
â”‚   â”œâ”€â”€ upload_to_s3.py           # Uploads Parquet files to AWS S3
â”‚   â”œâ”€â”€ cleanup_local_data.py     # Deletes local processed files
â”‚   â”œâ”€â”€ config.py                 # Configuration file (API keys, paths)
â”‚   â”œâ”€â”€ spark_session.py          # Initializes PySpark session
â”‚   â”œâ”€â”€ s3_client_object.py       # Creates AWS S3 client using Boto3
â”‚
â”‚â”€â”€ data/                         # Data storage
â”‚   â”œâ”€â”€ processed_data/           # Transformed data saved as Parquet
â”‚   â”œâ”€â”€ raw_data/                 # Raw JSON data fetched from Spotify API
â”‚
â”‚â”€â”€ db/                           # Database-related files (if needed)
â”‚â”€â”€ logs/                         # Airflow logs and pipeline logs
â”‚â”€â”€ plugins/                      # Custom Airflow plugins (if needed)
â”‚
â”‚â”€â”€ test/                         # Testing framework
â”‚   â”œâ”€â”€ test.py                   # Unit tests for ETL scripts
â”‚
â”‚â”€â”€ dockerfile                     # Docker container setup
â”‚â”€â”€ docker-compose.yaml             # Docker Compose configuration
â”‚â”€â”€ requirements.txt                # Dependencies
â”‚â”€â”€ README.md                       # Project documentation
               
```

---

## ğŸ“œ **Module Breakdown**  

### 1ï¸âƒ£ **Data Extraction - `fetch_spotify_data.py`**  
- Connects to **Spotify API** using **Spotipy**  
- Fetches playlist track metadata (**song name, artist, album, popularity, etc.**)  
- Saves data as a **JSON file** in the `raw_data/` directory  

### 2ï¸âƒ£ **Data Transformation - `spotify_transformation.py`**  
- Uses **Apache Spark** to process raw JSON  
- Extracts structured **albums, artists, and songs information**  
- Cleans and **deduplicates** data before storage  

### 3ï¸âƒ£ **Parquet Storage - `parquet_writer.py`**  
- Converts transformed data into **Parquet format** for optimized storage  
- Stores data in the `processed_data/` directory  

### 4ï¸âƒ£ **Uploading to AWS S3 - `upload_to_s3.py`**  
- Uses **Boto3** to upload Parquet files to an **S3 bucket**  
- Runs **parallel uploads** using **multithreading** for efficiency  

### 5ï¸âƒ£ **Cleanup Process - `cleanup_local_data.py`**  
- Deletes **raw & processed data** from local storage after successful upload  

### 6ï¸âƒ£ **Configuration & Setup - `config.py`**  
- Stores **Spotify API credentials**  
- Stores **AWS S3 bucket details**  
- Defines local directories for **raw and processed data**  

### 7ï¸âƒ£ **S3 Client Initialization - `s3_client_object.py`**  
- Creates a reusable **AWS S3 client** using **Boto3**  
- Provides secure access to **S3 for uploading files**  

### 8ï¸âƒ£ **Spark Session Manager - `spark_session.py`**  
- Initializes **Apache Spark session** for data processing  
- Configures **Spark parameters** for performance tuning  

---

## ğŸ› ï¸ **Tech Stack**  

ğŸ”¹ **Apache Airflow** â†’ ETL orchestration  
ğŸ”¹ **Apache Spark** â†’ Data processing & transformation  
ğŸ”¹ **Spotipy** â†’ Spotify API wrapper  
ğŸ”¹ **Boto3** â†’ AWS S3 integration  
ğŸ”¹ **Parquet** â†’ Optimized data storage format  
ğŸ”¹ **Python** â†’ Core programming language  

---

## âš™ï¸ **Installation & Setup**  

### 1ï¸âƒ£ **Clone Repository**  

```bash
https://github.com/shivatadha/Spotify-ETL-Pipeline-using-Apache-Airflow.git
cd spotify-etl-airflow
```

### 2ï¸âƒ£ **Install Dependencies**  

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ **Set Up Airflow**  

```bash
export AIRFLOW_HOME=~/airflow
airflow db init
airflow scheduler &
airflow webserver
```

### 4ï¸âƒ£ **Set Up Configuration**  
- Open **`config.py`** and update your **Spotify API credentials** and **AWS S3 credentials**  

---

## ğŸ—ï¸ **Running the Pipeline**  

1ï¸âƒ£ Open **Airflow UI** â†’ [http://localhost:8080](http://localhost:8080)  
2ï¸âƒ£ **Trigger the `spotify_etl_dag`** to start the pipeline  

---

## ğŸ“Œ **Future Improvements**  

âœ”ï¸ **Integrate with PostgreSQL** for analytics  
âœ”ï¸ **Add real-time streaming** using **Kafka**  
âœ”ï¸ **Automate Spotify API token refresh**  

---

